# プロジェクト名: {{ project.name }}
# バージョン: {{ project.version }}

x-gpu: &gpu
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: all
            capabilities: [gpu]

x-ray: &ray
{% if services.ray.cpu.enabled %}
  image: {{ services.ray.cpu.image }}
{% if services.ray.cpu.build and services.ray.cpu.build.enabled %}
  build:
    context: {{ services.ray.cpu.build.context }}
    dockerfile: {{ services.ray.cpu.build.dockerfile }}
    target: {{ services.ray.cpu.build.target }}
{% endif %}
{% elif services.ray.gpu.enabled %}
  image: {{ services.ray.gpu.image }}
{% if services.ray.gpu.build and services.ray.gpu.build.enabled %}
  build:
    context: {{ services.ray.gpu.build.context }}
    dockerfile: {{ services.ray.gpu.build.dockerfile }}
    target: {{ services.ray.gpu.build.target }}
{% endif %}
{% endif %}
  restart: unless-stopped
  networks:
    - ray-network

services:
{% if services.health.enabled %}
  health:
    image: python:3.12-alpine
    container_name: {{ project.name }}-health
    volumes:
      - ../template/health-ep.sh:/app/health-ep.sh:ro
    command: ["python3", "/app/health-ep.sh"]
    ports:
      - "0.0.0.0:{{ services.health.port }}:8080"
    networks:
      - ray-network
{% endif %}

{% if services.ray.cpu.enabled %}
  ray-cpu:
    <<: *ray
{% if services.ray.cpu.build and services.ray.cpu.build.enabled %}
    image: {{ services.ray.cpu.image }}
    build:
      context: {{ services.ray.cpu.build.context }}
      dockerfile: {{ services.ray.cpu.build.dockerfile }}
      target: {{ services.ray.cpu.build.target }}
{% endif %}
    container_name: {{ project.name }}-ray-cpu
    hostname: ray-cpu
    restart: "no"
    shm_size: '2gb'
    environment:
      - RAY_HEAD_PORT={{ services.ray.cpu.head_port }}
      - RAY_DASHBOARD_PORT={{ services.ray.cpu.dashboard_port }}
      - RAY_CLIENT_PORT={{ services.ray.cpu.client_port }}
    {% if services.ray.cpu.node_ip_address %}
      - NODE_IP_ADDRESS={{ services.ray.cpu.node_ip_address }}
    {% endif %}
    {% if services.ray.cpu.node_manager_port %}
      - NODE_MANAGER_PORT={{ services.ray.cpu.node_manager_port }}
    {% endif %}
    {% if services.ray.cpu.object_manager_port %}
      - OBJECT_MANAGER_PORT={{ services.ray.cpu.object_manager_port }}
    {% endif %}
    {% if services.ray.cpu.min_worker_port %}
      - MIN_WORKER_PORT={{ services.ray.cpu.min_worker_port }}
    {% endif %}
    {% if services.ray.cpu.max_worker_port %}
      - MAX_WORKER_PORT={{ services.ray.cpu.max_worker_port }}
    {% endif %}
      - HEAD_WHITELIST={{ nodes.head_whitelist | join(' ') }}
      - HEAD_ADDRESS_CFG={{ nodes.head_address or '' }}
      - HEALTH_URL={{ nodes.health_service.url }}
      - HEALTH_TIMEOUT={{ nodes.health_service.timeout }}
      - DISCOVERY_TIMEOUT={{ nodes.cluster.discovery_timeout }}
      - WAIT_FOR_HEAD={{ nodes.cluster.wait_for_head }}
{% if services.ray.cpu.cpus %}
      - RAY_NUM_CPUS={{ services.ray.cpu.cpus }}
{% endif %}
{% if services.ray.cpu.build and services.ray.cpu.build.enabled %}
      - USE_CONDA_ENV=1
{% endif %}
    volumes:
      - ../template/ray-ep.sh:/app/ray-ep.sh:ro
    user: root
{% if services.ray.cpu.build and services.ray.cpu.build.enabled %}
    entrypoint: ["/bin/bash", "-c", "apt-get update -qq && apt-get install -y -qq curl > /dev/null 2>&1 && /app/ray-ep.sh"]
{% else %}
    entrypoint: ["/bin/bash", "-c", "apt-get update -qq && apt-get install -y -qq curl > /dev/null 2>&1 && su ray -c '/app/ray-ep.sh'"]
{% endif %}
{% if services.health.enabled %}
    depends_on:
      - health
{% endif %}
{% if services.ray.cpu.cpus or services.ray.cpu.memory %}
    deploy:
      resources:
        limits:
{% if services.ray.cpu.cpus %}
          cpus: "{{ services.ray.cpu.cpus }}"
{% endif %}
{% if services.ray.cpu.memory %}
          memory: {{ services.ray.cpu.memory }}
{% endif %}
{% endif %}
    ports:
      - "{{ services.ray.cpu.dashboard_port }}:{{ services.ray.cpu.dashboard_port }}"  # Ray Dashboard
      - "{{ services.ray.cpu.client_port }}:{{ services.ray.cpu.client_port }}"  # Ray Client
      - "{{ services.ray.cpu.head_port }}:{{ services.ray.cpu.head_port }}"  # Head process
    profiles:
      - ray-cpu
{% endif %}

{% if host.has_gpu and services.ray.gpu.enabled %}
  ray-gpu:
    <<: [*ray, *gpu]
{% if services.ray.gpu.build and services.ray.gpu.build.enabled %}
    image: {{ services.ray.gpu.image }}
    build:
      context: {{ services.ray.gpu.build.context }}
      dockerfile: {{ services.ray.gpu.build.dockerfile }}
      target: {{ services.ray.gpu.build.target }}
{% endif %}
    container_name: {{ project.name }}-ray-gpu
    hostname: ray-gpu
    restart: "no"
    shm_size: '2gb'
    environment:
      - RAY_HEAD_PORT={{ services.ray.gpu.head_port }}
      - RAY_DASHBOARD_PORT={{ services.ray.gpu.dashboard_port }}
      - RAY_CLIENT_PORT={{ services.ray.gpu.client_port }}
    {% if services.ray.gpu.node_ip_address %}
      - NODE_IP_ADDRESS={{ services.ray.gpu.node_ip_address }}
    {% endif %}
    {% if services.ray.gpu.node_manager_port %}
      - NODE_MANAGER_PORT={{ services.ray.gpu.node_manager_port }}
    {% endif %}
    {% if services.ray.gpu.object_manager_port %}
      - OBJECT_MANAGER_PORT={{ services.ray.gpu.object_manager_port }}
    {% endif %}
    {% if services.ray.gpu.min_worker_port %}
      - MIN_WORKER_PORT={{ services.ray.gpu.min_worker_port }}
    {% endif %}
    {% if services.ray.gpu.max_worker_port %}
      - MAX_WORKER_PORT={{ services.ray.gpu.max_worker_port }}
    {% endif %}
      - HEAD_WHITELIST={{ nodes.head_whitelist | join(' ') }}
      - HEAD_ADDRESS_CFG={{ nodes.head_address or '' }}
      - HEALTH_URL={{ nodes.health_service.url }}
      - HEALTH_TIMEOUT={{ nodes.health_service.timeout }}
      - DISCOVERY_TIMEOUT={{ nodes.cluster.discovery_timeout }}
      - WAIT_FOR_HEAD={{ nodes.cluster.wait_for_head }}
      - RAY_NUM_GPUS=1
{% if services.ray.gpu.cpus %}
      - RAY_NUM_CPUS={{ services.ray.gpu.cpus }}
{% endif %}
{% if services.ray.gpu.build and services.ray.gpu.build.enabled %}
      - USE_CONDA_ENV=1
{% endif %}
    volumes:
      - ../template/ray-ep.sh:/app/ray-ep.sh:ro
    user: root
{% if services.ray.gpu.build and services.ray.gpu.build.enabled %}
    entrypoint: ["/bin/bash", "-c", "apt-get update -qq && apt-get install -y -qq curl > /dev/null 2>&1 && /app/ray-ep.sh"]
{% else %}
    entrypoint: ["/bin/bash", "-c", "apt-get update -qq && apt-get install -y -qq curl > /dev/null 2>&1 && su - ray -c '/app/ray-ep.sh'"]
{% endif %}
{% if services.health.enabled %}
    depends_on:
      - health
{% endif %}
{% if services.ray.gpu.cpus or services.ray.gpu.memory %}
    deploy:
      resources:
        limits:
{% if services.ray.gpu.cpus %}
          cpus: "{{ services.ray.gpu.cpus }}"
{% endif %}
{% if services.ray.gpu.memory %}
          memory: {{ services.ray.gpu.memory }}
{% endif %}
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
{% endif %}
    ports:
      - "{{ services.ray.gpu.dashboard_port }}:{{ services.ray.gpu.dashboard_port }}"  # Ray Dashboard
      - "{{ services.ray.gpu.client_port }}:{{ services.ray.gpu.client_port }}"  # Ray Client
      - "{{ services.ray.gpu.head_port }}:{{ services.ray.gpu.head_port }}"  # Head process
    profiles:
      - ray-gpu
{% endif %}

{% if services.redis.enabled %}
  ray-redis:
    image: {{ services.redis.image }}
    container_name: {{ project.name }}-redis
    restart: unless-stopped
    ports:
      - "{{ services.redis.port }}:6379"
    networks:
      - ray-network
{% endif %}

{% if services.mlflow.enabled %}
  mlflow:
    image: {{ services.mlflow.image }}
    container_name: {{ project.name }}-mlflow
    restart: unless-stopped
    command: mlflow server --host 0.0.0.0 --port 5000
    ports:
      - "{{ services.mlflow.port }}:5000"
    environment:
      - MLFLOW_BACKEND_STORE_URI=postgresql://{{ services.mlflow.postgres.user }}:{{ services.mlflow.postgres.password }}@mlflow-postgres/{{ services.mlflow.postgres.database }}
      - MLFLOW_ARTIFACT_ROOT=/mlflow/artifacts
    volumes:
      - {{ volumes.mlflow_data }}:/mlflow/artifacts
{% if services.mlflow.postgres.enabled %}
    depends_on:
      - mlflow-postgres
{% endif %}
    networks:
      - ray-network
{% endif %}

{% if services.mlflow.postgres.enabled %}
  mlflow-postgres:
    image: {{ services.mlflow.postgres.image }}
    container_name: {{ project.name }}-mlflow-postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: {{ services.mlflow.postgres.user }}
      POSTGRES_PASSWORD: {{ services.mlflow.postgres.password }}
      POSTGRES_DB: {{ services.mlflow.postgres.database }}
    volumes:
      - {{ volumes.postgres_data }}:/var/lib/postgresql/data
    networks:
      - ray-network
{% endif %}

{% if services.marimo.enabled %}
  marimo:
    image: {{ services.marimo.image }}
{% if services.marimo.build and services.marimo.build.enabled %}
    build:
      context: {{ services.marimo.build.context }}
      dockerfile: {{ services.marimo.build.dockerfile }}
      target: {{ services.marimo.build.target }}
{% endif %}
    container_name: {{ project.name }}-marimo
    restart: unless-stopped
{% if services.marimo.build and services.marimo.build.enabled %}
    environment:
      - USE_CONDA_ENV=1
    command: ["conda", "run", "-n", "py", "marimo", "edit", "--host", "0.0.0.0", "--port", "8080", "/workspace"]
{% endif %}
    ports:
      - "{{ services.marimo.port }}:8080"
    volumes:
      - ../:/workspace
{% if services.ray.cpu.enabled %}
    depends_on:
      - ray-cpu
{% endif %}
    networks:
      - ray-network
{% endif %}

networks:
  ray-network:
    driver: bridge
    ipam:
      config:
        - subnet: {{ network.subnet }}

volumes:
  mlflow_data:
  postgres_data:
  ray_data:
